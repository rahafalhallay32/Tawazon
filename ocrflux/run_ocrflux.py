# ocrflux/run_ocrflux.py

import os
from PIL import Image
from transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText
from pdf2image import convert_from_path

# -------------------
# Paths and settings
# -------------------
MODEL_PATH = "ChatDOC/OCRFlux-3B"
PDF_FOLDER = "../data/pdfs"
TEXT_FOLDER = "../data/text_files"
EXTRACT_PROMPT = (
    "You are analyzing an Arabic presentation slide. Extract all visible text exactly as it appears, preserving the reading direction from right to left. "
    "Detect section headers and any other main titles, and format them as <h1> headers. "
    "Format subpoints, phrases, and descriptive statements under each header using bullet points or paragraphs. "
    "Do not repeat or hallucinate content. Retain the Arabic language, and clearly structure the output in a way that reflects how a person would read and understand the slide visually."
)

# -------------------
# Load OCRFlux model
# -------------------
print("üì¶ Loading OCRFlux model and processor...")
model = AutoModelForImageTextToText.from_pretrained(
    MODEL_PATH,
    torch_dtype="auto",
    device_map="auto",
)
model.eval()

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
processor = AutoProcessor.from_pretrained(MODEL_PATH)

# -------------------
# OCR a single image
# -------------------
def ocr_image(image_path, max_new_tokens=4096):
    image = Image.open(image_path)
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": [
                {"type": "image", "image": f"file://{image_path}"},
                {"type": "text", "text": EXTRACT_PROMPT},
            ],
        },
    ]
    text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    inputs = processor(text=[text], images=[image], padding=True, return_tensors="pt")
    inputs = inputs.to(model.device)

    output_ids = model.generate(
        **inputs, temperature=0.0, max_new_tokens=max_new_tokens, do_sample=False
    )

    generated_ids = [
        output_ids[len(input_ids):]
        for input_ids, output_ids in zip(inputs.input_ids, output_ids)
    ]

    output_text = processor.batch_decode(
        generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
    )
    return output_text[0]

# -------------------
# Process all PDFs
# -------------------
def process_pdfs():
    os.makedirs(TEXT_FOLDER, exist_ok=True)
    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith(".pdf")]

    for pdf_file in pdf_files:
        pdf_path = os.path.join(PDF_FOLDER, pdf_file)
        text_output_path = os.path.join(TEXT_FOLDER, pdf_file.replace(".pdf", ".txt"))

        print(f"üìÑ Converting {pdf_file} to images...")
        images = convert_from_path(pdf_path)

        full_text = ""
        for i, image in enumerate(images):
            image_path = f"temp_page_{i}.png"
            image.save(image_path)

            print(f"üîç OCR page {i+1}/{len(images)}...")
            text = ocr_image(image_path, max_new_tokens=4096)
            full_text += f"\n\n===== Page {i+1} =====\n\n{text}"

            os.remove(image_path)

        with open(text_output_path, "w", encoding="utf-8") as f:
            f.write(full_text)

        print(f"‚úÖ Saved: {text_output_path}\n")

if __name__ == "__main__":
    process_pdfs()
